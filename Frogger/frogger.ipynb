{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phowlett\\OneDrive - Cherry Creek School District\\Desktop\\AIClassFrogger\\.conda\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "# REQUIRES PYTHON 3.12.0\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "from ale_py import ALEInterface, roms\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceFrame(frame): # Make frame B&W and slim it down to reduce compute time\n",
    "    gar=cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)\n",
    "    crap=gar[12:-30, 7:-7]\n",
    "    resized = cv2.resize(crap, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "    return resized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"CNN architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSnapshotSystem(size=100000): # Make the memory space for snapshots and history tracking\n",
    "    return deque(maxlen=size)\n",
    "\n",
    "def addSnapshot(snapshots): # TODO I need to finish this \n",
    "    # snapshots.append((state, action, reward, next_state, done))\n",
    "    pass\n",
    "\n",
    "\n",
    "def trainModel(episodes=1000 # How many episodes to run\n",
    "               maxStep=50000 # Max ammount of steps the frog can take before giving up\n",
    "               \n",
    "               ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(episodes=1000, #How many episodes to run\n",
    "               maxStep=50000): #Uhh i think this is how many steps it can take\n",
    "    action_size = env.action_space.n\n",
    "    model = build_model(action_size)\n",
    "    target_model = build_model(action_size)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    memory = makeSnapshotSystem()\n",
    "    \n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    batch_size = 32\n",
    "    update_target_freq = 10 \n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state_raw, _ = env.reset()\n",
    "        state = reduceFrame(state_raw)\n",
    "        state = np.expand_dims(state, axis=-1) \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < maxStep:\n",
    "            step += 1\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, action_size - 1)\n",
    "            else:\n",
    "                q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            next_state_raw, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = reduceFrame(next_state_raw)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(memory) >= batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                states = np.array([m[0] for m in minibatch])\n",
    "                actions = np.array([m[1] for m in minibatch])\n",
    "                rewards = np.array([m[2] for m in minibatch])\n",
    "                next_states = np.array([m[3] for m in minibatch])\n",
    "                dones = np.array([m[4] for m in minibatch])\n",
    "\n",
    "                target_qs = target_model.predict(next_states, verbose=0)\n",
    "                targets = model.predict(states, verbose=0)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    if dones[i]:\n",
    "                        targets[i][actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        targets[i][actions[i]] = rewards[i] + gamma * np.max(target_qs[i])\n",
    "\n",
    "                model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if episode % update_target_freq == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward:.2f}, Epsilon = {epsilon:.3f}\")\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
